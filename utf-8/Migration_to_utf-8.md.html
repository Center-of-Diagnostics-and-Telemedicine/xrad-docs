<meta charset="utf-8">
<meta lang="ru">

**Проект изменения работы с кодировками строк в XRAD**

# Проект перехода #

Переход требует достаточно большого объема работы, предлагается процедура перехода в
несколько этапов.

1. Проверить, ограничивается ли список мест, где для string и [const] char * существенно
  используется кодировка ANSI и нет возможности в принципе или дешевым способом установить
  кодировку UTF-8:
  <ul>
  <li>`fopen` — имя файла;</li>
  <li>`fstream`, `ifstream`, `ofstream` — имя файла в конструкторах и в open;</li>
  <li>обмен данными (чтение и запись файлов) в кодировке win-1251 или в системной кодировке.</li>
  </ul>

  Если так, то идем дальше.

2. В CompilerSpecific объявить `std::fopen`, `std::basic_ifstream` и аналогичные deprecated.

3. В namespace xrad в часть CompilerSpecific добавить:
  <ul>
  <li>функцию `xrad::fopen` по стандартному прототипу;</li>
  <li>классы `xrad::basic_fstream` и компания по необходимости (наследуются от `std::basic_fstream`
    и т.д.).</li>
  </ul>

  В добавленных функциях и классах в местах, где происходит работа с именем файла (для классов
  потоков — в конструкторе и в open) сделать
  преобразование имени файла в Юникод и вызов соответствующей юникодовой функции (конструктора).
  Преобразование на данном этапе должно делаться функцией convert_to_wstring, из системной
  кодировки Windows, чтобы не нарушить работоспособность имеющегося кода. Пока мы просто
  заготавливаем переходники, которые нам понадобятся в дальнейшем.

  Внутри namespace xrad эти добавленные функции и классы подхватятся автоматически вместо
  функций и классов из namespace std.

  Вне namespace xrad нужно будет явно записывать вызов этих реализаций (`xrad::fopen`,
  `xrad::fstream`...).
  В стандартном случае, при наличии директив `using namespace xrad;` и `using namespace std;`
  использование `fopen`, `fstream` без
  явного указания namespace приведет к ошибке компиляции.

  Внести необходимые изменения в код (добавить `xrad::`).

  Убедиться, что не осталось пропущенных обращений к заменяемым функциям и классам.

  Пример заголовочного файла с реализациями `fopen` и `fstream`:
~~~~
#include &lt;fstream&gt;
namespace xrad
{

FILE *fopen(const char* filename, const char* mode);

template &lt;class CharT, class Traits = std::char_traits&lt;CharT&gt;&gt;
class basic_ifstream_: public std::basic_ifstream&lt;CharT, Traits&gt;
{
 public:
   using std::basic_ifstream&lt;CharT, Traits&gt;::basic_ifstream;
   explicit basic_ifstream_( const char* filename, std::ios_base::openmode mode = ios_base::in);
   explicit basic_ifstream_( const std::string& filename, std::ios_base::openmode mode = ios_base::in);
};

using ifstream = basic_ifstream&lt;char&gt;;

}
~~~~

3. Если есть обмен данными не в Юникоде (например, в win-1251):
  1. Сделать функции типа string_to_code_page или convert_to_code_page и обратные им. Если
    требуется (или может понадобиться в будущем) использование кодировки, отличной от системной,
    то кодировка должна задаваться явно,
    как в `string_to_wstring_MS`. В остальном они пока должны дублировать функциональность
    конвертера в `string` (для типа `string` они пока возвращают переданную строку без изменений).
  2. Во всех местах, где происходит обмен данными не в Юникоде, вставить вызов функций
    преобразования.
  3. Делать отдельный класс для строк в неизвестной 8-битной кодировке считаю лишним. Мест, где
    возникают такие строки, немного. Нужно просто кодировать этот факт в названии переменных и в
    комментариях к ним.
    А для обработки эти строки всё равно, я думаю, всегда будут преобразовываться в UTF-8 или
    UTF-32. UTF-16 предлагаю использовать только для ввода-вывода, вызовов API, не поддерживающего
    UTF-8, и, возможно, каких-то случаев, когда она дает существенные плюсы по сравнению с UTF-8 и
    UTF-32.

    Данные, у которых может быть разной разрядность кодировки (8-битная кодировка, UTF-16 LE,
    UTF-16 BE), можно хранить вообще не в строковых классах. Например, в `DataArray&lt;char&gt;`.

4. Кодировку по умолчанию для string сделать UTF-8.
  1. Задать директиву `#pragma execution_character_set("utf-8")` в одном из базовых compiler
    specific заголовочных файлов.
  2. Изменить работу преобразований string в и из юникодовых типов строк.
    1. Если ранее были введены функции string_to_code_page, то их реализация должна стать похожей
      на старую реализацию конвертера в string. Если такие функции введены не были, то
      существующие функции конвертации в string (по крайней мере те, где содержится базовая
      функциональность) следует переименовать со словом code_page; возможно, переместить их в
      архив.
  3. Внести, если необходимо, изменения в GUI-функции.

5. Избавиться от `wstring` везде, где возможно. Оставить `wstring`, `wchar_t` только в обращениях
  к API, привязанному к Windows, во внутренних скрытых частях библиотеки.

  В интерфейсных функциях оставить только `string` (теперь они используют кодировку UTF-8 ).

  Строковые константы `L"..."` заменить на `u8"..."`.

  Договориться, что все «интерфейсные» строки должны начинаться с префикса `u8`.

  <p class="note">Я бы хотел, чтобы "интерфейсные" (потенциально локализуемые) строки можно было
  отличить от "технических" (нелокализуемых) по каким-то формальным критериям. Один из возможных
  способов — дать им некий префикс, не меняющий содержимое строки. Таким префиксом может быть
  `u8` (`u8"Тест"`) или `""` (`"""Тест"`). Можно использовать механизм user-defined literals
  ( https://en.cppreference.com/w/cpp/language/user_literal ), если он нормально поддерживается
  в MSVC2015. Использование макросов типа `T("строка")` считаю нежелательным.</p>



# Устройство кодировки UTF-8 #

https://ru.wikipedia.org/wiki/UTF-8

Кодировка UTF-8 предназначена для кодирования символов Unicode (коды 0—0x10FFFF) в виде
последовательности 8-битных чисел (октетов). Один символ Unicode кодируется последовательностью от
1 до 4 октетов в зависимости от кода символа.

<p class="note">В ранних версиях стандарта допускалось кодирование символов в диапазоне
0—0x7FFFFFFF и длина кода одного символа могла составлять от 1 до 6 октетов. Но позже, из
соображений совместимости с кодировкой UTF-16, диапазон символов Unicode сузили до 0-0x10FFFF,
последовательности длины 5 и 6 в UTF-8 стали не нужны.</p>

В UTF-8 символы с кодами 0—0x7F (ASCII-символы) кодируются одним октетом со значением, совпадающим
с кодом символа 0—0x7F. Остальные символы кодируются последовательностями из 2—4 октетов.
Начальные октеты последовательностей длины 2 имеют коды 0xC0—0xDF, длины 3 — 0xE0—0xEF, длины 4 —
0xF0—0xF7. Промежуточные и конечные октеты последовательностей имеют коды из одного и того же
диапазона 0x80—0xВF. (Полное описание алгоритма кодирования см., например, в Википедии.)

Таким образом, для любого октета строки в кодировке UTF-8 можно по его коду определить,
является ли он ASCII-символом, начальным байтом последовательности или промежуточным/конечным
байтом последовательности.

В отличие от некоторых других многобайтовых кодировок (например, 54936 для китайского языка), в
UTF-8 ASCII-символы не могут быть частью какой-то последовательности, кодирующей другие символы.
В многооктетных последовательностях все октеты имеют коды &gt;= 0x80.
Более того, последовательность октетов, кодирующая любой символ,
не может встретиться в последовательности, кодирующей один или несколько других символов. В этом
большое преимущество этой кодировки.

Нахождение границ символов возможно как с начала, так и с конца, и даже с середины строки.
Впрочем, для определения количества символов нужно просканировать всю строку, знания длины
недостаточно. Для нахождения N-го символа также необходимо последовательно проанализировать все
октеты, предшествующие этому символу.



# Преобразование текста между кодировками #

При преобразовании кодировок нужно учитывать, что некоторые последовательности байтов являются
недопустимыми в UTF-8. Функции преобразования из UTF-8 в другие кодировки могут обрабатывать
строки с такими последовательностями по-разному. Вот несколько возможных стратегий:
1. кидать исключение или возвращать признак ошибки;
2. возвращать пустую строку;
3. возвращать строку, в которой недопустимые последовательности заменены символом по умолчанию
  (например, `'?'`);
4. возвращать строку, в которой недопустимые последовательности просто пропущены;
5. возвращать строку, в которой недопустимые последовательности заменены некоторыми кодовыми
  последователностями, позволяющими затем выполнить обратное преобразование без потерь данных.

С точки зрения безопасности (security) наиболее правильными являются стратегии 1 и 3. В XRAD
используется комбинированная стратегия. Функции преобразования заменяют недопустимые
последовательности символом по умолчанию, некоторые из них также устанавливают флаг ошибки.

Следует иметь в виду, что кодировки UTF-16 и UTF-32 в бинарном представлении также могут содержать
недопустимые коды:
- UTF-32: коды вне диапазона 0—0x10FFFF;
- UTF-16: части суррогатных пар по отдельности.



# Работа в кодировке UTF-8 #

Для большинства правильно написанных операций работа с текстом в кодировке UTF-8 не
отличается от работы в любой другой кодировке. Но нужно держать в голове, что один символ может
кодироваться последовательностью из нескольких байтов.



## Посимвольный разбор ##

При посимвольном разборе нужно учитывать, что один символ может представляться последовательностью
от 1 до 4 байтов. Это означает, что невозможно обратиться к N-му символу по индексу (индекс зависит
от длин последовательностей, кодирующих предшествующие символы в строке), нельзя выделять
подстроки в произвольных позициях.

Кодировка UTF-16 в плане посимвольного разбора мало чем лучше UTF-8. В UTF-16 один символ может
кодироваться двумя словами.

Для посимвольного разбора удобно использовать UTF-32. Здесь одному символу Юникода соответствует
один символ UTF-32.

Однако независимо от используемой кодировки нужно иметь в виду, что в некоторых языках не принято
обрезать слова на стыке некоторых символов, которые в паре считаются одной буквой.
Примерно так, как если бы в русском языке буква Ы была не отдельным символом, а записывалась парой
из двух символов типа Ъ и I.

Так что UTF-32 тоже не решает проблему на 100%.



## Поиск символа ##

- Поиск ASCII-символа в кодировке UTF-8 сводится к поиску одного байта (или char в С++),
  как если бы вся строка была в ASCII-кодировке.
- Поиск произвольного символа сводится к поиску подстроки длины от 1 до 4. На первый взгляд это
  значительно медленнее, чем поиск одного символа. Но при использовании оптимизированных
  алгоритмов поиска подстроки это может быть ненамного медленнее поиска одного символа.

Поиск можно производить как с начала, так и с конца строки.



## Поиск подстроки (слова) ##

Поиск подстроки в UTF-8 не отличается от поиска в ASCII-кодировке (анализировать соседние символы
найденной подстроки не нужно).

Поиск можно производить как с начала, так и с конца строки.



## Сортировка ##

> "Кстати, интересный вопрос. std::set&amp;lt;string&amp;gt;, насколько я мог понять, сортируется по
> алфавиту. Как будут обстоять дела со строками utf8?"

Кодировка UTF-8 устроена таким образом, что строки сортируются в порядке возрастания юникодовых
значений символов. Так же, как и в UTF-32.

В этом плане UTF-16 хуже, в ней порядок сортировки другой. Из-за суррогатных пар юникодовые
символы в диапазоне U+10000...U+10FFFF будут при сортировке находиться между U+BFFF и U+E000.

В UTF-8 для ASCII (английский язык) и для русских букв, исключая ё, получается сортировка в
алфавитном порядке с учетом регистра. Для французского из-за букв с аксентами уже будет не так.

На самом деле алфавитная сортировка — целая наука.
Один и тот же набор "слов" (последовательностей символов) в разных языках может быть отсортирован
по-разному.
А в немецком, например, насколько я знаю, существует по крайней мере два немного отличающихся друг
от друга способа сортировки.

В WinAPI существует функция LCMapStringEx, с помощью которой строку можно преобразовать в ключ
сортировки — последовательность байтов, которую можно сравнивать с ключом сортировки другой строки
на больше—меньше. При формировании ключа нужно указать, хотим ли учитывать регистр символов и ещё
какие-то особенности письма.
https://msdn.microsoft.com/en-us/library/windows/desktop/dd318144(v=vs.85).aspx



## Автоматическое определение кодировки ##

Иногда возникают задачи автоопределения кодировки текстового файла. Например, когда приходит
большое количество файлов в разных кодировках.

Одним из алгоритмов опеределния кодировки является алгоритм частотного анализа символов.
Содержимое файла последовательно конвертируется из разных кодировок в некоторую фиксированную,
результат конвертации подвергается анализу частоты вхождения некоторых символов. (Эта реализация
отнюдь не самая быстрая, но зато простая.)

Для этого алгоритма в качестве кодировки, в которую конвертируется содержимое файла, можно
предложить использовать UTF-32.

И здесь как раз то место, для которого нужно будет сделать в функции перекодировки
"8-bit encoding" string -> utf-8 параметр enum code_page, у этого enum должен быть член cp_1251
(для win-1251, сейчас это есть в StringConverters_MS.h).
Нам здесь нужна именно кодировка win-1251, а не текущая системная.
Но для текущей системной тоже сделать член enum.



<style>
body {
  max-width:100%;
}
.note {
  font-size:80%;
}
.comment {
  color:gray;
  font-size:80%;
}
</style>
<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script>window.markdeepOptions = {mode: 'markdeep', detectMath: false};</script><script src="markdeep.js"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
